import os
import time
import numpy as np
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from sklearn.metrics.pairwise import cosine_similarity

# === í™˜ê²½ ì„¤ì • === #
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # ë³‘ë ¬ ê²½ê³  ì œê±°
load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
embedding_model_name = os.getenv("EMBEDDING_MODEL")

if not openai_api_key:
    raise ValueError("âŒ OPENAI_API_KEY not found in .env file")
if not embedding_model_name:
    raise ValueError("âŒ EMBEDDING_MODEL not found in .env file")

# === ì„ë² ë”© ë° DB ë¡œë”© === #
embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)

db = FAISS.load_local("vectordb", embedding, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_kwargs={"k": 5})

# === LLM ì„¤ì • === #
llm = ChatOpenAI(
    temperature=0.2,
    model_name="gpt-4.1",
    openai_api_key=openai_api_key
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=True
)

# === í‰ê°€ í•¨ìˆ˜ === #
def cosine_sim(text1, text2, embedding_model):
    emb1 = embedding_model.embed_documents([text1])[0]
    emb2 = embedding_model.embed_documents([text2])[0]
    return cosine_similarity([emb1], [emb2])[0][0]

def evaluate_accuracy(retrieved, expected, embedding_model, threshold=0.4):
    tp, fn = 0, 0
    for e in expected:
        if any(cosine_sim(r, e, embedding_model) >= threshold for r in retrieved):
            tp += 1
        else:
            fn += 1
    fp = len(retrieved) - tp
    prec = tp / (tp + fp) if (tp + fp) else 0
    rec = tp / (tp + fn) if (tp + fn) else 0
    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) else 0
    return prec, rec, f1

def evaluate_query(query, expected, embedding_model, retriever, threshold=0.35):
    start = time.time()
    results = retriever.invoke(query)
    elapsed = time.time() - start
    retrieved = [doc.page_content for doc in results[:10]]
    prec, rec, f1 = evaluate_accuracy(retrieved, expected, embedding_model, threshold)
    return elapsed, prec, rec, f1, retrieved

def evaluate_answer(query, expected, chain):
    try:
        start = time.time()
        result = chain.invoke({"query": query})
        elapsed = time.time() - start
        answer = result["result"]
        sim = max(cosine_sim(answer, e, embedding) for e in expected)
        return elapsed, answer, sim
    except Exception as e:
        return 0, str(e), 0

# === ì‹¤í–‰ === #
if __name__ == "__main__":
    testset = [
        {
            "query": "í•™ìŠµ ì „ëµì˜ ì¢…ë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "expected": [
                "í•™ìŠµ ì „ëµì—ëŠ” ì¸ì§€ ì „ëµ, ë©”íƒ€ì¸ì§€ ì „ëµ, ìì› ê´€ë¦¬ ì „ëµì´ í¬í•¨ë©ë‹ˆë‹¤.",
                "ë©”íƒ€ì¸ì§€ ì „ëµì€ ìì‹ ì˜ í•™ìŠµì„ ê³„íš, ëª¨ë‹ˆí„°ë§, í‰ê°€í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤."
            ]
        },
        {
            "query": "í•™ìƒì˜ ìê¸°ì¡°ì ˆ í•™ìŠµ ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.",
            "expected": [
                "ìê¸°ì¡°ì ˆ í•™ìŠµì€ ëª©í‘œ ì„¤ì •, ìê¸° ëª¨ë‹ˆí„°ë§, ìê¸° í‰ê°€ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.",
                "í•™ìƒë“¤ì€ ëª©í‘œë¥¼ ì„¤ì •í•˜ê³  í•™ìŠµ ê³¼ì • ì¤‘ ìì‹ ì„ ì ê²€í•˜ë©° í‰ê°€í•©ë‹ˆë‹¤."
            ]
        }
    ]

    for i, case in enumerate(testset, 1):
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ {i}: {case['query']}")

        db_time, prec, rec, f1, retrieved = evaluate_query(case["query"], case["expected"], embedding, retriever)
        print(f"ğŸ” ê²€ìƒ‰ ì‹œê°„: {db_time:.2f}s | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}")

        ans_time, answer, sim = evaluate_answer(case["query"], case["expected"], qa_chain)
        print(f"ğŸ§  ì‘ë‹µ ì‹œê°„: {ans_time:.2f}s | ìœ ì‚¬ë„: {sim:.3f}\nğŸ“ ë‹µë³€ ìš”ì•½: {answer[:100]}...")
