import os
import requests
import xml.etree.ElementTree as ET
import pdfplumber
from datetime import datetime
from openai import OpenAI
from textwrap import wrap
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings

# =========================
# ì„¤ì •
# =========================
PDF_DIR = "/Users/jeong/AI/learnmate/data/papers"
SUMMARY_DIR = "/Users/jeong/AI/learnmate/data/abstracts"
VECTOR_STORE_DIR = "/Users/jeong/AI/learnmate/vectordb"

KEYWORD = "learning strategy of student"
START_INDEX = 0 
MAX_RESULTS = 50
TITLE_SLICE = 60
CHUNK_SIZE = 3000

# =========================
# í™˜ê²½ ì„¤ì •
# =========================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# =========================
# í•¨ìˆ˜ ì •ì˜
# =========================

def sanitize_filename(text):
    return "".join(c for c in text if c.isalnum() or c in " ._-").rstrip()

def fetch_arxiv(keyword=KEYWORD, start_index=START_INDEX, max_results=MAX_RESULTS):
    os.makedirs(PDF_DIR, exist_ok=True)
    os.makedirs(SUMMARY_DIR, exist_ok=True)

    print(f"ğŸ“¡ arXivì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")

    base_url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": f"title:{keyword}",
        "start": start_index,
        "max_results": max_results,
        "sortBy": "submittedDate",
        "sortOrder": "descending"
    }

    try:
        res = requests.get(base_url, params=params)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ arXiv ìš”ì²­ ì‹¤íŒ¨: {e}")
        return

    root = ET.fromstring(res.text)
    ns = {"atom": "http://www.w3.org/2005/Atom"}

    entries = root.findall("atom:entry", ns)
    print(f"ğŸ” {len(entries)}í¸ ë…¼ë¬¸ ê²€ìƒ‰ë¨\n")

    for entry in entries:
        title = entry.find("atom:title", ns).text.strip()
        summary = entry.find("atom:summary", ns).text.strip()
        pdf_url = entry.find("atom:id", ns).text.strip().replace("abs", "pdf")

        filename_base = sanitize_filename(title[:TITLE_SLICE])
        md_path = os.path.join(SUMMARY_DIR, f"{filename_base}.md")
        pdf_path = os.path.join(PDF_DIR, f"{filename_base}.pdf")

        # ìš”ì•½ ì €ì¥
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(f"# {title}\n\n")
            f.write(f"**Abstract:**\n{summary}\n\n")
            f.write(f"[arXiv ì›ë¬¸ ë§í¬]({pdf_url})\n")
        print(f"ğŸ“ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {md_path}")

        # PDF ë‹¤ìš´ë¡œë“œ
        try:
            pdf_res = requests.get(pdf_url + ".pdf")
            pdf_res.raise_for_status()
            with open(pdf_path, "wb") as f:
                f.write(pdf_res.content)
            print(f"âœ… PDF ì €ì¥ ì™„ë£Œ: {pdf_path}")
        except Exception as e:
            print(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

def extract_text_from_pdf(pdf_path):
    try:
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
        return text.strip()
    except Exception as e:
        print(f"âŒ PDF ë¡œë”© ì˜¤ë¥˜ ({pdf_path}): {e}")
        return ""

def chunk_text(text, max_chars=CHUNK_SIZE):
    return wrap(text, width=max_chars)

def summarize_text(text):
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” êµìœ¡ ê´€ë ¨ ë…¼ë¬¸ì„ ìš”ì•½í•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ì•¼. í•µì‹¬ì„ ëª…í™•íˆ ìš”ì•½í•´ì¤˜."},
                {"role": "user", "content": f"ë‹¤ìŒ ë…¼ë¬¸ì„ ìš”ì•½í•´ì¤˜:\n{text}"}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ìµœì¢… ìš”ì•½ ì‹¤íŒ¨: {e}")
        return ""

def save_summary_to_md(title, summary):
    filename = "".join(c for c in title[:TITLE_SLICE] if c.isalnum() or c in " ._-").rstrip().replace(" ", "_") + ".md"
    path = os.path.join(SUMMARY_DIR, filename)
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"# {title}\n\n{summary}\n")
    print(f"âœ… ìµœì¢… ìš”ì•½ ì €ì¥ ì™„ë£Œ: {path}")

# =========================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# =========================

def run_pipeline():
    fetch_arxiv()

    all_docs = []

    for filename in os.listdir(PDF_DIR):
        if not filename.endswith(".pdf"):
            continue

        pdf_path = os.path.join(PDF_DIR, filename)
        print(f"\nğŸ“„ ë…¼ë¬¸ ì²˜ë¦¬ ì‹œì‘: {filename}")

        text = extract_text_from_pdf(pdf_path)
        if not text:
            continue

        # Chunk ë¶„í•  â†’ DB ì €ì¥ìš©
        chunks = chunk_text(text)
        for idx, chunk in enumerate(chunks):
            doc = Document(page_content=chunk, metadata={"source": filename, "chunk_idx": idx})
            all_docs.append(doc)

        # ìµœì¢… ìš”ì•½ â†’ .md ì €ì¥
        summary = summarize_text(text)
        if summary:
            save_summary_to_md(filename.replace(".pdf", ""), summary)

        # PDF ì‚­ì œ
        os.remove(pdf_path)
        print(f"ğŸ—‘ PDF ì‚­ì œ ì™„ë£Œ: {pdf_path}")

    # ëª¨ë“  ë¬¸ì„œ FAISS ì €ì¥
    if all_docs:
        print("\nğŸ§  ëª¨ë“  chunkë¥¼ ë²¡í„°í™” ì¤‘...")
        embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        db = FAISS.from_documents(all_docs, embedding)
        db.save_local(VECTOR_STORE_DIR)
        print(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {VECTOR_STORE_DIR}")

# ì‹¤í–‰
if __name__ == "__main__":
    print(f"â± ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    run_pipeline()
