import os
import time
import requests
import pdfplumber
from openai import OpenAI
from datetime import datetime
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from textwrap import TextWrapper

from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_huggingface import HuggingFaceEmbeddings

# =========================
# ÏÑ§Ï†ï
# =========================
BASE_URL = "https://eric.ed.gov"
SEARCH_URL = "https://eric.ed.gov/?q=learning+strategy+OR+study+skill+OR+metacognition+of+student&ft=on&ff1=dtySince_2024"

PDF_DIR = "data/papers"
SUMMARY_DIR = "data/abstracts"
VECTOR_STORE_DIR = "vectordb"
TITLE_SLICE = 60
CHUNK_SIZE = 5000  # Max chunk size in tokens

# =========================
# ÌôòÍ≤Ω ÏÑ§Ï†ï
# =========================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# =========================
# Ìï®Ïàò Ï†ïÏùò
# =========================
def sanitize_filename(text):
    return "".join(c for c in text if c.isalnum() or c in " ._-").rstrip()

def fetch_paper_links(max_pages=10):
    print(f"üîç ERIC ÎÖºÎ¨∏ Î™©Î°ù Í∞ÄÏ†∏Ïò§Îäî Ï§ë...")
    paper_links = []
    page_num = 0
    next_url = SEARCH_URL

    while next_url and page_num < max_pages:
        page_num += 1
        print(f"üìÑ ÌéòÏù¥ÏßÄ {page_num} ÏöîÏ≤≠ Ï§ë: {next_url}")
        try:
            res = requests.get(next_url)
            res.raise_for_status()
        except Exception as e:
            print(f"‚ö†Ô∏è ÏöîÏ≤≠ Ïã§Ìå® (ÌéòÏù¥ÏßÄ {page_num}): {e}")
            break

        soup = BeautifulSoup(res.text, "html.parser")

        entries = soup.select(".r_i")
        links = [urljoin(BASE_URL, entry.select_one("a")["href"]) for entry in entries if entry.select_one("a")]
        paper_links.extend(links)

        next_button = soup.find("a", string="Next Page ¬ª")
        if next_button and next_button.get("href"):
            next_url = urljoin(BASE_URL, next_button["href"])
        else:
            next_url = None

        time.sleep(1)

    print(f"üìö Ï¥ù {len(paper_links)}Ìé∏ ÎÖºÎ¨∏ ÏàòÏßë ÏôÑÎ£å (ÏµúÎåÄ {max_pages}ÌéòÏù¥ÏßÄ Í∏∞Ï§Ä)")
    return paper_links



def download_pdf(paper_url):
    eric_id = paper_url.split("id=")[-1]
    title = eric_id
    pdf_url = f"http://files.eric.ed.gov/fulltext/{eric_id}.pdf"
    filename = sanitize_filename(title[:TITLE_SLICE]) + ".pdf"
    path = os.path.join(PDF_DIR, filename)

    if not os.path.exists(path):
        print(f"‚¨áÔ∏è Îã§Ïö¥Î°úÎìú Ï§ë: {eric_id}")
        try:
            pdf_res = requests.get(pdf_url)
            pdf_res.raise_for_status()
            with open(path, "wb") as f:
                f.write(pdf_res.content)
            print(f"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {path}")
            return eric_id, path
        except Exception as e:
            print(f"‚ùå PDF Îã§Ïö¥Î°úÎìú Ïã§Ìå® ({eric_id}): {e}")
            return None, None
    else:
        print(f"üì¶ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï®: {path}")
        return eric_id, path

def extract_text_from_pdf(pdf_path):
    try:
        with pdfplumber.open(pdf_path) as pdf:
            return "".join(page.extract_text() or "" for page in pdf.pages).strip()
    except Exception as e:
        print(f"‚ùå PDF Î°úÎî© Ïã§Ìå®: {e}")
        return ""

def chunk_text(text, max_tokens=CHUNK_SIZE):
    # Estimate token count by considering a rough average of 4 characters per token
    max_chars = max_tokens * 4  # approximate
    wrapper = TextWrapper(width=max_chars, break_long_words=False)
    return wrapper.wrap(text)

def summarize_text(text):
    try:
        # Split text into smaller chunks based on token length
        chunks = chunk_text(text)
        summaries = []
        
        for chunk in chunks:
            # Check token length for each chunk to avoid exceeding the limit
            response = client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                    {"role": "system", "content": "ÎÑàÎäî ÍµêÏú° Í¥ÄÎ†® ÎÖºÎ¨∏ÏùÑ ÏöîÏïΩÌïòÎäî ÌïúÍµ≠Ïñ¥ Ï†ÑÎ¨∏Í∞ÄÏïº. ÌïµÏã¨ ÎÇ¥Ïö©ÏùÑ ÏöîÏïΩÌï¥Ï§ò."},
                    {"role": "user", "content": f"Îã§Ïùå ÎÖºÎ¨∏ÏùÑ 10000Îã®Ïñ¥ ÎÇ¥Ïô∏Î°ú ÏöîÏïΩ:\n{chunk}"}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            summaries.append(response.choices[0].message.content.strip())
        
        return " ".join(summaries)  # Combine all summaries from chunks
    except Exception as e:
        print(f"‚ùå ÏöîÏïΩ Ïã§Ìå®: {e}")
        return ""

def save_summary_to_md(title, summary):
    filename = sanitize_filename(title[:TITLE_SLICE]) + ".md"
    path = os.path.join(SUMMARY_DIR, filename)
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"# {title}\n\n{summary}\n")
    print(f"‚úÖ ÏöîÏïΩ Ï†ÄÏû• ÏôÑÎ£å: {path}")
    return path

def run_pipeline():
    os.makedirs(PDF_DIR, exist_ok=True)
    os.makedirs(SUMMARY_DIR, exist_ok=True)

    links = fetch_paper_links(max_pages=10)
    all_docs = []

    for link in links:
        try:
            title, pdf_path = download_pdf(link)
            if not pdf_path:
                continue

            # ÏöîÏïΩ .md ÌååÏùºÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎ©¥ Ïä§ÌÇµ
            md_filename = sanitize_filename(title[:TITLE_SLICE]) + ".md"
            md_path = os.path.join(SUMMARY_DIR, md_filename)
            if os.path.exists(md_path):
                print(f"‚è© Ïù¥ÎØ∏ ÏöîÏïΩÎê®: {md_path}")
                continue

            text = extract_text_from_pdf(pdf_path)
            os.remove(pdf_path)
            print(f"üóë PDF ÏÇ≠Ï†ú ÏôÑÎ£å: {pdf_path}")

            if not text:
                print(f"‚è© ÌÖçÏä§Ìä∏ ÏóÜÏùå: {title}")
                continue

            summary = summarize_text(text)
            if not summary:
                continue

            save_summary_to_md(title, summary)
            chunks = chunk_text(summary)

            for idx, chunk in enumerate(chunks):
                doc = Document(page_content=chunk, metadata={"source": md_path, "chunk_idx": idx})
                all_docs.append(doc)

            time.sleep(2)

        except Exception as e:
            print(f"‚ùå Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            continue

    if all_docs:
        print("\nüß† Î™®Îì† ÏöîÏïΩÏùÑ Î≤°ÌÑ∞Ìôî Ï§ë...")
        embedding = HuggingFaceEmbeddings(model_name=os.getenv("EMBEDDING_MODEL"))
        db = FAISS.from_documents(all_docs, embedding)
        db.save_local(VECTOR_STORE_DIR)
        print(f"‚úÖ Î≤°ÌÑ∞ DB Ï†ÄÏû• ÏôÑÎ£å: {VECTOR_STORE_DIR}")
    else:
        print("üì≠ Î≤°ÌÑ∞ÌôîÌï† Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.")

# Ïã§Ìñâ
if __name__ == "__main__":
    print(f"\n‚è± Ïã§Ìñâ ÏãúÍ∞Å: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    run_pipeline()
