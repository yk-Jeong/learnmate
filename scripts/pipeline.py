import os
import requests
import pdfplumber
from datetime import datetime
from openai import OpenAI
from textwrap import wrap
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings

# =========================
# ì„¤ì •
# =========================
PDF_DIR = "/Users/jeong/AI/learnmate/data/papers"
SUMMARY_DIR = "/Users/jeong/AI/learnmate/data/abstracts"
VECTOR_STORE_DIR = "/Users/jeong/AI/learnmate/vectordb"

KEYWORD = 'learning strategy or metacognition of student'
START_INDEX = 0
MAX_RESULTS = 200
TITLE_SLICE = 60
CHUNK_SIZE = 3000
CATEGORY = "education" 

# =========================
# í™˜ê²½ ì„¤ì •
# =========================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# =========================
# í•¨ìˆ˜ ì •ì˜
# =========================
def sanitize_filename(text):
    return "".join(c for c in text if c.isalnum() or c in " ._-").rstrip()

def fetch_semantic_scholar(keyword=KEYWORD, start_index=START_INDEX, max_results=MAX_RESULTS, category=CATEGORY):
    os.makedirs(PDF_DIR, exist_ok=True)
    os.makedirs(SUMMARY_DIR, exist_ok=True)

    print(f"ğŸ“¡ Semantic Scholarì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")

    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {
        "query": f"{keyword} AND field:{category} AND has_pdf:true",  # PDFê°€ ìˆëŠ” ë…¼ë¬¸ë§Œ ê²€ìƒ‰
        "offset": start_index,
        "limit": max_results,
        "sort": "relevance"
    }

    try:
        res = requests.get(base_url, params=params)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ Semantic Scholar ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []

    papers = res.json().get('data', [])
    print(f"ğŸ” {len(papers)}í¸ ë…¼ë¬¸ ê²€ìƒ‰ë¨\n")

    for paper in papers:
        title = paper.get("title", "").strip()
        abstract = paper.get("abstract", "").strip()
        pdf_url = paper.get("url", "")  # PDF URLì„ ì¶”ì¶œ

        if not pdf_url:  # pdf_urlì´ ë¹„ì–´ìˆìœ¼ë©´ ì²˜ë¦¬
            print(f"âš ï¸ PDF URLì´ ì—†ìŠµë‹ˆë‹¤: {title}")
            continue  # PDF URLì´ ì—†ìœ¼ë©´ ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.

        filename_base = sanitize_filename(title[:TITLE_SLICE])
        md_path = os.path.join(SUMMARY_DIR, f"{filename_base}.md")
        pdf_path = os.path.join(PDF_DIR, f"{filename_base}.pdf")

        # ìš”ì•½ ì €ì¥
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(f"# {title}\n\n")
            f.write(f"**Abstract:**\n{abstract}\n\n")
            f.write(f"[Semantic Scholar ì›ë¬¸ ë§í¬]({pdf_url})\n")
        print(f"ğŸ“ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {md_path}")

        # PDF ë‹¤ìš´ë¡œë“œ
        try:
            print(f"PDF URL: {pdf_url}")  # URL ì¶œë ¥ í™•ì¸
            pdf_res = requests.get(pdf_url)
            pdf_res.raise_for_status()
            with open(pdf_path, "wb") as f:
                f.write(pdf_res.content)
            print(f"âœ… PDF ì €ì¥ ì™„ë£Œ: {pdf_path}")
        except Exception as e:
            print(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

    return papers  # ê²€ìƒ‰ëœ ë…¼ë¬¸ ëª©ë¡ ë°˜í™˜


def extract_text_from_pdf(pdf_path):
    try:
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
        return text.strip()
    except Exception as e:
        print(f"âŒ PDF ë¡œë”© ì˜¤ë¥˜ ({pdf_path}): {e}")
        return ""

def chunk_text(text, max_chars=CHUNK_SIZE):
    return wrap(text, width=max_chars)

def summarize_text(text):
    try:
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” êµìœ¡ ê´€ë ¨ ë…¼ë¬¸ì„ ìš”ì•½í•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ì•¼. í•µì‹¬ì„ ëª…í™•íˆ ìš”ì•½í•´ì¤˜."},
                {"role": "user", "content": f"ë‹¤ìŒ ë…¼ë¬¸ì„ 2000ë‹¨ì–´ë¡œ ìš”ì•½í•´ì¤˜:\n{text}"}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ìµœì¢… ìš”ì•½ ì‹¤íŒ¨: {e}")
        return ""

def save_summary_to_md(title, summary):
    filename = "".join(c for c in title[:TITLE_SLICE] if c.isalnum() or c in " ._-").rstrip().replace(" ", "_") + ".md"
    path = os.path.join(SUMMARY_DIR, filename)
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"# {title}\n\n{summary}\n")
    print(f"âœ… ìµœì¢… ìš”ì•½ ì €ì¥ ì™„ë£Œ: {path}")

# =========================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# =========================
def run_pipeline():
    all_papers = fetch_semantic_scholar()

    all_docs = []

    for filename in os.listdir(PDF_DIR):
        if not filename.endswith(".pdf"):
            continue

        pdf_path = os.path.join(PDF_DIR, filename)
        print(f"\nğŸ“„ ë…¼ë¬¸ ì²˜ë¦¬ ì‹œì‘: {filename}")

        text = extract_text_from_pdf(pdf_path)
        if not text:
            print(f"âš ï¸ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ â†’ {filename}")
            continue

        # Chunk ë¶„í•  â†’ DB ì €ì¥ìš©
        chunks = chunk_text(text)
        for idx, chunk in enumerate(chunks):
            doc = Document(page_content=chunk, metadata={"source": filename, "chunk_idx": idx})
            all_docs.append(doc)

        # ìµœì¢… ìš”ì•½ â†’ .md ì €ì¥
        summary = summarize_text(text)
        if summary:
            save_summary_to_md(filename.replace(".pdf", ""), summary)

        # PDF ì‚­ì œ
        os.remove(pdf_path)
        print(f"ğŸ—‘ PDF ì‚­ì œ ì™„ë£Œ: {pdf_path}")

    # ëª¨ë“  ë¬¸ì„œ FAISS ì €ì¥
    if all_docs:
        print("\nğŸ§  ëª¨ë“  chunkë¥¼ ë²¡í„°í™” ì¤‘...")
        embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        db = FAISS.from_documents(all_docs, embedding)
        db.save_local(VECTOR_STORE_DIR)
        print(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {VECTOR_STORE_DIR}")

# ì‹¤í–‰
if __name__ == "__main__":
    print(f"â± ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    run_pipeline()
