import os
import streamlit as st
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
import argparse

os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

pdf_dir = "/Users/jeong/AI/learnmate/data/abstracts"
file_count = len([f for f in os.listdir(pdf_dir) if os.path.isfile(os.path.join(pdf_dir, f))])


# í™˜ê²½ ì„¤ì •
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY not found in .env file")

# ë²¡í„° DB ë¡œë“œ
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
try:
    db = FAISS.load_local("vectordb", embedding, allow_dangerous_deserialization=True)
    retriever = db.as_retriever(search_kwargs={"k": 2})
except Exception as e:
    raise ValueError(f"âŒ FAISS ë¡œë”© ì˜¤ë¥˜: {e}")

# LLM ì„¤ì •
llm = ChatOpenAI(
    temperature=0.2,
    model_name="gpt-4.1", 
    openai_api_key=openai_api_key
)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
system_prompt = (
    "your persona: ì¤‘ê³ ë“±í•™ìƒì˜ í•™ì—… ê³ ë¯¼ì„ ë“¤ì–´ì£¼ëŠ” ë”°ëœ»í•˜ê³  ì „ëµì ì¸ ëŒ€í•™ìƒ ë©˜í† "
    "ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ, ì„ ë°°ê°€ í›„ë°°ì—ê²Œ ì¡°ì–¸í•˜ë“¯ ì¹œì ˆí•˜ê²Œ ì œì•ˆ"
    "ë¨¼ì € ì§ˆë¬¸ìì˜ ìƒí™©ì— ê³µê°í•˜ê³ , ì´ì–´ì„œ [ì „ëµ â†’ ì‹¤ì²œ] êµ¬ì¡°ë¡œ ë‹µë³€"
    "í…ìŠ¤íŠ¸+ë©€í‹°ëª¨ë‹¬ ì¶œë ¥(ì˜ˆì‹œì¹´ë“œ ë“±ë“±)"
    "í•´ìš”ì²´ë¡œ ì‘ì„±"
    "/Users/jeong/AI/learnmate/data/abstracts ë‚´ì˜ ë°ì´í„°ì— ê¸°ë°˜í•  ê²ƒ"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "ì§ˆë¬¸: {question}\n\nğŸ“ ì°¸ê³  ë¬¸ì„œ:\n{context}")
])

# QA ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}
)

# ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜
def ask_rag(question):
    try:
        result = qa_chain.invoke({"query": question})
        return result["result"]
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# Streamlit UI
st.set_page_config(page_title="Learnmate", layout="wide")
st.title("Learnmate: í•™ìŠµ ê³ ë¯¼ì„ ë‚˜ëˆ„ì–´ìš”!")
st.markdown(f"{file_count}í¸ì˜ ì‹¤ì œ ë…¼ë¬¸ì— ê¸°ë°˜í•´ì„œ, ë‹¹ì‹ ì˜ í•™ìŠµ ê³ ë¯¼ì„ ê³¼í•™ì ìœ¼ë¡œ í•´ì†Œí•´ ë“œë¦½ë‹ˆë‹¤.")

question = st.text_input("ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”? êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í• ìˆ˜ë¡, ìì„¸í•˜ê²Œ ëŒ€ë‹µí•´ ë“œë¦´ ìˆ˜ ìˆì–´ìš”!")

if question:
    with st.spinner("ëŒ€ë‹µì„ ê³ ë¯¼í•˜ëŠ” ì¤‘..."):
        answer = ask_rag(question)
        st.success("ì´ë ‡ê²Œ í•´ ë´ìš”!")
        st.write(answer)
